{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChromaGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KunalVaidya99/Image-Colorization-Using-Deep-Learning/blob/main/ChromaGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA8bcFbU0-VF",
        "outputId": "297e3e7b-4c19-4f9a-910a-9758ab4ed515",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CswEHXhU1YDR",
        "outputId": "13c81383-0e9f-4043-f3cb-5a2af828aa3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaoB1Kbu1iXX",
        "outputId": "980ad903-27d7-4661-dd2f-cd9c6a84439d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.layers import (Conv2D,Conv2DTranspose,UpSampling2D,MaxPooling2D,Input\n",
        "                          ,Activation,Concatenate,BatchNormalization,Flatten,Concatenate\n",
        "                          ,Dense,RepeatVector,Reshape)\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model,Sequential\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import load_img\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import random\n",
        "import progressbar\n",
        "from skimage.color import gray2rgb\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.applications import VGG16\n",
        "from keras.preprocessing.image import load_img,img_to_array"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWGloSyGgyav"
      },
      "source": [
        "Extracting Features for all images using VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKF4y_Cc2pe0",
        "outputId": "0e762439-a7b4-4e0a-cd40-06cd5e52b8d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def extract_features(directory):\n",
        "\n",
        "  model = VGG16(include_top=True,weights=\"imagenet\",classes=1000)\n",
        "  features = dict()\n",
        "  count = 0\n",
        "  for filename in os.listdir(directory):\n",
        "   \n",
        "    image = load_img(directory+filename,target_size=(224,224))\n",
        "    img_array = img_to_array(image)\n",
        "    img_array = img_array.reshape((1,224,224,3))\n",
        "    feature = model.predict(img_array)\n",
        "    features[filename] = feature\n",
        "    count = count + 1\n",
        "\n",
        "    if (count%1000==0):\n",
        "      print(\"Completed Feature Extraction Of \" + f\"{count}\") \n",
        "\n",
        "  return features\n",
        "\n",
        "directory = \"/content/gdrive/My Drive/val_256/\"\n",
        "features = extract_features(directory)       "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Completed Feature Extraction Of 1000\n",
            "Completed Feature Extraction Of 2000\n",
            "Completed Feature Extraction Of 3000\n",
            "Completed Feature Extraction Of 4000\n",
            "Completed Feature Extraction Of 5000\n",
            "Completed Feature Extraction Of 6000\n",
            "Completed Feature Extraction Of 7000\n",
            "Completed Feature Extraction Of 8000\n",
            "Completed Feature Extraction Of 9000\n",
            "Completed Feature Extraction Of 10000\n",
            "Completed Feature Extraction Of 11000\n",
            "Completed Feature Extraction Of 12000\n",
            "Completed Feature Extraction Of 13000\n",
            "Completed Feature Extraction Of 14000\n",
            "Completed Feature Extraction Of 15000\n",
            "Completed Feature Extraction Of 16000\n",
            "Completed Feature Extraction Of 17000\n",
            "Completed Feature Extraction Of 18000\n",
            "Completed Feature Extraction Of 19000\n",
            "Completed Feature Extraction Of 20000\n",
            "Completed Feature Extraction Of 21000\n",
            "Completed Feature Extraction Of 22000\n",
            "Completed Feature Extraction Of 23000\n",
            "Completed Feature Extraction Of 24000\n",
            "Completed Feature Extraction Of 25000\n",
            "Completed Feature Extraction Of 26000\n",
            "Completed Feature Extraction Of 27000\n",
            "Completed Feature Extraction Of 28000\n",
            "Completed Feature Extraction Of 29000\n",
            "Completed Feature Extraction Of 30000\n",
            "Completed Feature Extraction Of 31000\n",
            "Completed Feature Extraction Of 32000\n",
            "Completed Feature Extraction Of 33000\n",
            "Completed Feature Extraction Of 34000\n",
            "Completed Feature Extraction Of 35000\n",
            "Completed Feature Extraction Of 36000\n",
            "Completed Feature Extraction Of 37000\n",
            "Completed Feature Extraction Of 38000\n",
            "Completed Feature Extraction Of 39000\n",
            "Completed Feature Extraction Of 40000\n",
            "Completed Feature Extraction Of 41000\n",
            "Completed Feature Extraction Of 42000\n",
            "Completed Feature Extraction Of 43000\n",
            "Completed Feature Extraction Of 44000\n",
            "Completed Feature Extraction Of 45000\n",
            "Completed Feature Extraction Of 46000\n",
            "Completed Feature Extraction Of 47000\n",
            "Completed Feature Extraction Of 48000\n",
            "Completed Feature Extraction Of 49000\n",
            "Completed Feature Extraction Of 50000\n",
            "Completed Feature Extraction Of 51000\n",
            "Completed Feature Extraction Of 52000\n",
            "Completed Feature Extraction Of 53000\n",
            "Completed Feature Extraction Of 54000\n",
            "Completed Feature Extraction Of 55000\n",
            "Completed Feature Extraction Of 56000\n",
            "Completed Feature Extraction Of 57000\n",
            "Completed Feature Extraction Of 58000\n",
            "Completed Feature Extraction Of 59000\n",
            "Completed Feature Extraction Of 60000\n",
            "Completed Feature Extraction Of 61000\n",
            "Completed Feature Extraction Of 62000\n",
            "Completed Feature Extraction Of 63000\n",
            "Completed Feature Extraction Of 64000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6GuKhL-5-aT"
      },
      "source": [
        "import pickle\n",
        "f = open(\"/content/gdrive/My Drive/ImageColorization /chromaVGG16.pkl\",\"wb\")\n",
        "pickle.dump(features,f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoHf2gb8yFA_"
      },
      "source": [
        "f_new = open(\"/content/gdrive/My Drive/ImageColorization /chromaVGG16.pkl\",\"rb\")\n",
        "features_new = pickle.load(f_new)\n",
        "f_new.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYtomIhCJfZf"
      },
      "source": [
        "class DataGenerator():\n",
        "\n",
        "  def __init__(self,features,directory,dim=(224,224),batch_size=32,n_channels=3):\n",
        "    \n",
        "    self.directory = directory\n",
        "    self.dim = dim\n",
        "    self.batch_size=batch_size\n",
        "    self.n_channels = n_channels\n",
        "    self.features = features\n",
        "    \n",
        "\n",
        "  def epoch_end(self,filenames):\n",
        "    np.random.shuffle(filenames)\n",
        "\n",
        "  def generate_batch(self,step_number,filenames):\n",
        "    \n",
        "\n",
        "    batch_filenames = filenames[step_number*self.batch_size:(step_number+1)*self.batch_size]\n",
        "\n",
        "    gray_image = np.empty((self.batch_size,*self.dim,self.n_channels))\n",
        "    color_image = np.empty((self.batch_size,*self.dim,self.n_channels))\n",
        "    VGG16_target = np.empty((self.batch_size,1000))\n",
        "    count = 0\n",
        "\n",
        "\n",
        "    for filename in batch_filenames:\n",
        "      image = Image.open(self.directory +\"/\" + filename)\n",
        "      image = image.resize((224,224))\n",
        "\n",
        "\n",
        "      if (np.array(image).shape==(224,224,3)):\n",
        "        \n",
        "        gray = (np.array(image.convert(\"L\")))\n",
        "        gray_image[count] = gray2rgb(gray)/255\n",
        "        color_image[count] = np.array(image)/255\n",
        "        VGG16_target[count] = self.features[filename]\n",
        "        count = count + 1\n",
        "\n",
        "    return gray_image,color_image,VGG16_target    "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACE0LabGvNW5"
      },
      "source": [
        "def patch_discriminator(img_shape):\n",
        "\n",
        "  input_img = Input(shape=img_shape)\n",
        "\n",
        "  X = Conv2D(filters=64,kernel_size=(4,4),strides=(2,2),padding='same')(input_img)\n",
        "  X = BatchNormalization(axis=-1)(X)\n",
        "  X = LeakyReLU(alpha=0.2)(X)\n",
        "\n",
        "  X = Conv2D(filters=128,kernel_size=(4,4),strides=(2,2),padding='same')(X)\n",
        "  X = BatchNormalization(axis=-1)(X)\n",
        "  X = LeakyReLU(alpha=0.2)(X)\n",
        "\n",
        "  X = Conv2D(filters=256,kernel_size=(4,4),strides=(2,2),padding='same')(X)\n",
        "  X = BatchNormalization(axis=-1)(X)\n",
        "  X = LeakyReLU(alpha=0.2)(X)\n",
        "\n",
        "  X = Conv2D(filters=512,kernel_size=(4,4),strides=(1,1),padding='same')(X)\n",
        "  X = BatchNormalization(axis=-1)(X)\n",
        "  X = LeakyReLU(alpha=0.2)(X)\n",
        "\n",
        "  X = Conv2D(filters=1,kernel_size=(4,4),strides=(1,1),padding='same')(X)\n",
        " \n",
        "  model = Model(inputs=input_img,outputs=X)\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgukDnq_8Uv0",
        "outputId": "e1d70bb9-2ca9-4f4d-ac05-0d20018a066f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        " VGG16model = VGG16(include_top=False,weights=\"imagenet\",classes=1000)\n",
        " #for layer in VGG16model.layers:\n",
        "  # layer.trainable = False"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVp9NhQx8IZP"
      },
      "source": [
        "def generator(img_shape):\n",
        "\n",
        "  input_img = Input(shape=img_shape)\n",
        "  X = input_img\n",
        "\n",
        "  for layer in VGG16model.layers[1:-6]:\n",
        "    X = layer(X)\n",
        "  \n",
        "  VGG16_output = X \n",
        "  #print(VGG16_output.shape)\n",
        "  X = Conv2D(filters=512,kernel_size=(3,3),strides=(2,2),padding=\"same\")(X)\n",
        "  X = BatchNormalization(axis=-1)(X)\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  X = Conv2D(filters=512,kernel_size=(3,3),strides=(1,1),padding=\"same\")(X)\n",
        "  X = BatchNormalization(axis=-1)(X)\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  X = Conv2D(filters=512,kernel_size=(3,3),strides=(2,2),padding=\"same\")(X)\n",
        "  X = BatchNormalization(axis=-1)(X)\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  X = Conv2D(filters=512,kernel_size=(3,3),strides=(1,1),padding=\"same\")(X)\n",
        "  X = BatchNormalization(axis=-1)(X)\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  color_distribution = Flatten()(X)\n",
        "  color_distribution = Dense(units=4096)(color_distribution)\n",
        "  color_distribution = Dense(units=4096)(color_distribution)\n",
        "  color_distribution = Dense(units=1000,activation='softmax',name='colordistribution')(color_distribution)\n",
        "\n",
        "  fusion = Flatten()(X)\n",
        "  fusion = Dense(units=1024)(fusion)\n",
        "  fusion = Dense(units=512)(fusion)\n",
        "  fusion = Dense(units=256)(fusion)\n",
        "  fusion = RepeatVector(28*28)(fusion)\n",
        "  fusion = Reshape((28,28,256))(fusion)\n",
        "\n",
        "  encoder_output = Conv2D(filters=512,kernel_size=(3,3),strides=(1,1),padding=\"same\")(VGG16_output)\n",
        "  encoder_output = BatchNormalization(axis=-1)(encoder_output)\n",
        "  encoder_output = Activation('relu')(encoder_output)\n",
        "  encoder_output = Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding=\"same\")(VGG16_output)\n",
        "  encoder_output = BatchNormalization(axis=-1)(encoder_output)\n",
        "  encoder_output = Activation('relu')(encoder_output)\n",
        "\n",
        "  encoder_output = Concatenate(axis=-1)([encoder_output,fusion])\n",
        "\n",
        "  decoder = Conv2D(filters=256,kernel_size=(1,1),strides=(1,1),activation='relu',padding='same')(encoder_output)\n",
        "  decoder = Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),activation='relu',padding='same')(decoder)\n",
        "  decoder = UpSampling2D((2,2))(decoder)\n",
        "\n",
        "  decoder = Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),padding='same',activation='relu')(decoder)\n",
        "  decoder = Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),padding='same',activation='relu')(decoder)\n",
        "  decoder = UpSampling2D((2,2))(decoder)\n",
        "\n",
        "  decoder = Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',activation='relu')(decoder)\n",
        "  decoder = Conv2D(filters=3,kernel_size=(3,3),strides=(1,1),padding='same',activation='sigmoid')(decoder)\n",
        "  decoder = UpSampling2D((2,2))(decoder)\n",
        "\n",
        "\n",
        "  model = Model(inputs=input_img,outputs=[decoder,color_distribution])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YK3gdNdGsRz"
      },
      "source": [
        "def chromaGAN(img_shape,discriminator,generator):\n",
        "  X = Input(img_shape)\n",
        "  image,color_distribution = generator(X)\n",
        "  output = discriminator(image)\n",
        "  model = Model(inputs=X,outputs=[image,color_distribution,output])\n",
        "  return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhLxJUnFNhv-"
      },
      "source": [
        "chromaGAN = chromaGAN((224,224,3),patch_discriminator,generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16z-7oN3Mk20"
      },
      "source": [
        "opt = Adam(learning_rate=0.00002,beta_1=0.9,beta_2=0.999)\n",
        "patch_discriminator = patch_discriminator((224,224,3))\n",
        "patch_discriminator.compile(optimizer=opt,loss='binary_crossentropy',loss_weights=[1])\n",
        "generator = generator((224,224,3))\n",
        "patch_discriminator.trainable = False\n",
        "chromaGAN = chromaGAN((224,224,3),patch_discriminator,generator)\n",
        "chromaGAN.compile(optimizer=opt,loss=[\"mse\",keras.losses.kullback_leibler_divergence,'binary_crossentropy'],loss_weights=[5,0.003,0.1])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMleCSntUjuE"
      },
      "source": [
        "def summarize_performance(step,generator,gray_images,color_images,name = \"Colorizer\"):\n",
        "  n_samples = 5\n",
        "  \n",
        "\n",
        "  images,_ = generator.predict(gray_images)\n",
        "  for i in range(n_samples):\n",
        "    plt.subplot(3, n_samples, 1 +i)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(images[i])\n",
        "    plt.title(\"Predicted\")\n",
        "  \n",
        "  for i in range(n_samples):\n",
        "    plt.subplot(3, n_samples, 1 + n_samples + i)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(gray_images[i])\n",
        "    plt.title(\"Gray\")\n",
        "\n",
        "  for i in range(n_samples):\n",
        "    plt.subplot(3, n_samples, 1 + 2*n_samples+i)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(color_images[i])\n",
        "    plt.title(\"GT\")\n",
        "  \n",
        "\n",
        "  filename1 = '%s_generated_plot_%06d.png' % (name, (step+1))\n",
        "  plt.savefig(filename1)\n",
        "  plt.close()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIyYJMjank_o"
      },
      "source": [
        "def train(generator,discriminator,chromaGAN,batch_size,epochs):\n",
        "  \n",
        "  directory = \"/content/gdrive/My Drive/val_256/\"\n",
        "  n_steps = int(len(os.listdir(directory))/batch_size)\n",
        "  filenames = os.listdir(directory)\n",
        "  DataLoader = DataGenerator(features_new,directory,batch_size=batch_size)\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    DataLoader.epoch_end(filenames)\n",
        "    print(\"**********************EPOCH \"+f\"{epoch+1}\" + \" *********************\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    for step in progressbar.progressbar(range(n_steps)):\n",
        "\n",
        "      gray_batch,color_batch,VGG16batch = DataLoader.generate_batch(step,filenames)\n",
        "      \n",
        "      \n",
        "      gray_batch_gen,color_batch_gen,VGG16batch_gen = DataLoader.generate_batch(n_steps-step,filenames)\n",
        "\n",
        "      fake_images,_ = generator.predict(x=gray_batch)\n",
        "      real = np.ones((batch_size,28,28,1))\n",
        "      fake = np.zeros((batch_size,28,28,1))\n",
        "\n",
        "      d_loss1 = discriminator.train_on_batch(x=color_batch,y=real)\n",
        "      d_loss2 = discriminator.train_on_batch(x=fake_images,y=fake)\n",
        "      d_loss = [d_loss1,d_loss2]\n",
        "\n",
        "      g_loss = chromaGAN.train_on_batch(x=gray_batch_gen,y=[color_batch_gen,VGG16batch_gen,real])\n",
        "\n",
        "      if ((step+1)%50==0):\n",
        "        \n",
        "        idx1 = np.random.randint(0,n_steps)\n",
        "        gray_images,color_images,_ = DataLoader.generate_batch(idx1,filenames)\n",
        "        summarize_performance(step,generator,gray_images[:5],color_images[:5])\n",
        "        print(\"Step Number :\",step+1,sep=\"  \")\n",
        "        print(\"Generator Loss :\",g_loss,sep=\"  \")\n",
        "        print(\"Discriminator Loss :\",d_loss)\n",
        "        print(\"\\n\")\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKDoqJLKy8pV",
        "outputId": "dd0f4956-bc82-45fe-e0b1-c31c14e40af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        }
      },
      "source": [
        "train(generator,patch_discriminator,chromaGAN,batch_size=32,epochs=10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\rN/A% (0 of 2030) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "**********************EPOCH 1 *********************\n",
            "\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "  0% (1 of 2030) |                       | Elapsed Time: 0:00:24 ETA:  13:35:18/tensorflow-1.15.2/python3.6/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "  2% (50 of 2030) |                      | Elapsed Time: 0:01:37 ETA:   1:48:02"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step Number :  50\n",
            "Generator Loss :  [0.3759892, 0.007233697, 0.455129, 3.3845532]\n",
            "Discriminator Loss : [0.5794139, 0.06185101]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3% (71 of 2030) |                      | Elapsed Time: 0:02:07 ETA:   0:47:14"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-35c291bf924b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatch_discriminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchromaGAN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-dd095693b740>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(generator, discriminator, chromaGAN, batch_size, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogressbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogressbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mgray_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVGG16batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-3078a4da4be9>\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(self, step_number, filenames)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_filenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1856\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1858\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreducing_gap\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNEAREST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcneD1RfzVYT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}